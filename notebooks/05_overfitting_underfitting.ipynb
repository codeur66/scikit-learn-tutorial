{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting, Overfitting and Regularization\n",
    "\n",
    "In the previous notebook, we have seen that some model parameters have a\n",
    "strong impact on the cross-validated accuracy of a predictive model.\n",
    "\n",
    "The goal of this notebook is to deepen our understanding of this effect by\n",
    "identifying when a model is too constrained to properly fit the variations\n",
    "of the train dataset or too flexible leading the model to memorize the\n",
    "the training data points individually (including noise) without having to\n",
    "make any \"effort\" at capturing the repeatable structure that would make\n",
    "its decision function to generalize correctly to unseen test points.\n",
    "\n",
    "This notebook shows:\n",
    "* how to tell if a trained predictive model is overfitting or underfitting (or both);\n",
    "* how to evaluate the impact of regularization hyperparameters on generalization;\n",
    "* how to evaluate the impact of the training set size on overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")\n",
    "# Or use the local copy:\n",
    "# df = pd.read_csv('../datasets/adult-census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name].to_numpy()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns=[target_name, \"fnlwgt\", \"education-num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is loaded, we split it into a training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=0, train_size=5000, test_size=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('binary', OneHotEncoder(drop=\"first\"),\n",
    "     ['sex']),\n",
    "    ('categorical', OneHotEncoder(handle_unknown='ignore'),\n",
    "     ['workclass', 'education', 'marital-status',\n",
    "      'occupation', 'relationship', 'race', 'native-country']),\n",
    "    ('numeric', PowerTransformer(),\n",
    "     ['age', 'capital-gain', 'capital-loss', 'hours-per-week']),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(df_train)\n",
    "df_preprocessed_train = preprocessor.transform(df_train)\n",
    "df_preprocessed_test = preprocessor.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "knn_model = make_pipeline(\n",
    "    TruncatedSVD(n_components=10),\n",
    "    KNeighborsClassifier()\n",
    ")\n",
    "param_name = \"kneighborsclassifier__n_neighbors\"\n",
    "param_range = [1, 5, 10, 50, 100, 500]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    knn_model,\n",
    "    df_preprocessed_train, target_train,\n",
    "    param_name=param_name,\n",
    "    param_range=param_range,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=2)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with k-Nearest Neighbors\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.78, 1.)\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\")\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_name = \"C\"\n",
    "param_range = [0.01, 0.1, 1, 10, 100]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    LogisticRegression(),\n",
    "    df_preprocessed_train, target_train,\n",
    "    param_name=\"C\",\n",
    "    param_range=param_range,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=2)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with Logistic Regression\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.8, 1.)\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\")\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1)\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "    \n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    plt.plot(train_sizes, test_scores_mean, label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "plot_learning_curve(LogisticRegression(solver=\"lbfgs\", max_iter=1000, C=1),\n",
    "                    \"Learning Curve for Logistic Regression\",\n",
    "                    df_preprocessed_train, target_train,\n",
    "                    ylim=(0.8, 1.), n_jobs=2,\n",
    "                    cv=StratifiedShuffleSplit(n_splits=20, test_size=0.2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
